"""
æµ‹è¯•extract_thinking_contentå‡½æ•°åœ¨çœŸå®LLMè¾“å‡ºä¸Šçš„æ•ˆæœ
"""
import os
import sys
from pathlib import Path
import time
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))


os.environ["USE_LOCAL_QWEN_FOR_EVAL"] = "true"
os.environ["LOCAL_QWEN_MODEL"] = "/mnt/tanka/models/Qwen2.5-32B-Instruct"
os.environ["VLLM_API_BASE"] = "http://10.119.21.75:9001/v1"


from verl.utils.kg_rewards import extract_thinking_content, get_model_name, extract_tag_content

def call_vllm_qwen(prompt: str, model_name: str = None, max_tokens: int = 1024) -> str:
    """
    è°ƒç”¨æœ¬åœ°å·²éƒ¨ç½²çš„vllm Qwenæ¨¡å‹æœåŠ¡ï¼ˆæ™®é€šæ–‡æœ¬å“åº”æ–¹å¼ï¼‰
    """
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹åç§°ï¼Œä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
    model_name = os.getenv("LOCAL_QWEN_MODEL", "/mnt/tanka/models/Qwen2.5-32B-Instruct")
        
    try:
        from openai import OpenAI
        
        # ä»ç¯å¢ƒå˜é‡è·å–APIåŸºç¡€URLï¼Œé»˜è®¤ä¸ºæœ¬åœ°vLLMæœåŠ¡
        api_base = os.getenv("VLLM_API_BASE", "http://10.119.21.75:9001/v1")
        
        print(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {model_name}")
        print(f"APIåŸºç¡€URL: {api_base}")
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯è¿æ¥åˆ°vLLMæœåŠ¡
        client = OpenAI(
            api_key="EMPTY",  # vLLMæœåŠ¡é€šå¸¸ä¸éœ€è¦APIå¯†é’¥
            base_url=api_base,
        )
        
        # è°ƒç”¨èŠå¤©æ¥å£
        chat_response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            top_p=0.9,
            max_tokens=max_tokens,
            timeout=30,
        )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = chat_response.choices[0].message.content
        
        return generated_text
    except Exception as e:
        print(f"vLLMè°ƒç”¨å¤±è´¥: {str(e)}")
        return ""

def test_think_tag_generation():
    """æµ‹è¯•LLMç”Ÿæˆå¸¦æœ‰thinkæ ‡ç­¾çš„å†…å®¹å¹¶æå–"""
    prompts = [
        # çŸ¥è¯†å›¾è°±æå–ä»»åŠ¡çš„æç¤º
        """Chunk Text:
[msg_id: 0] 2025-03-18T16:43:47 Huazheng WU: å¤§å®¶å¥½ï¼Œä»Šå¤©æœåŠ¡ç«¯å·²å‡çº§Google Gemini2.0çš„è¯­éŸ³æ¶ˆæ¯è½¬æ–‡å­—åŠŸèƒ½ï¼ˆæ„Ÿè°¢@Yize CHEN @Yuchen ZANG æ¨èGeminiä½œä¸ºè°ƒç ”æ–¹å‘ï¼‰ï¼Œæ¬¢è¿ä½“éªŒå¹¶åé¦ˆä½¿ç”¨æ„Ÿå—ï¼

åœ¨è°ƒç ”å¯¹æ¯”ä¼ ç»Ÿä¾›åº”å•†ï¼ˆGoogle Speechã€ç§‘å¤§è®¯é£ã€Microsoftã€Fanoï¼‰ä¸AIå¤§æ¨¡å‹ï¼ˆGladiaã€AssemblyAIã€Deepgramã€Geminiï¼‰åï¼Œæˆ‘ä»¬ä»å‡†ç¡®æ€§ã€å“åº”é€Ÿåº¦ã€æˆæœ¬ã€æ€§èƒ½ã€è‡ªåŠ¨è¯­è¨€è¯†åˆ«ã€éŸ³é¢‘æ ¼å¼æ”¯æŒç­‰å¤šä¸ªç»´åº¦è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ï¼ŒGeminiåœ¨ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰æ–¹é¢è¡¨ç°çªå‡ºã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒäº†è¯­éŸ³æƒ…ç»ªæ£€æµ‹ï¼Œå¹¶é€šè¿‡emojiç›´è§‚å‘ˆç°æƒ…ç»ªçŠ¶æ€ã€‚Google Speechè¯­éŸ³è½¬æ–‡å­—å°†ä½œä¸ºå¤‡ç”¨é€šé“ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ã€‚
[msg_id: 1] 2025-03-18T22:14:33 Tianqiao Chen: æˆ‘æ¥è¯•è¯•
[msg_id: 2] 2025-03-18T22:58:28 Tianqiao Chen: è°ç»™æˆ‘å‘ä¸€æ®µé•¿ä¸€ç‚¹çš„è¯­éŸ³ï¼šï¼‰
[msg_id: 3] 2025-03-18T23:00:47 Huazheng WU: sent an audio
[msg_id: 4] 2025-03-18T23:04:14 Hua ZHANG: sent an audio
[msg_id: 5] 2025-03-18T23:05:35 Hua ZHANG: è¯´çš„æ¯”è¾ƒé›¶ç¢å«ç³Šï¼Œè½¬å‡ºæ¥è¿˜æ˜¯æ¯”è¾ƒè¿˜åŸçš„
[msg_id: 6] 2025-03-18T23:05:37 Tianqiao Chen: åˆå¿«åˆå¥½
[msg_id: 7] 2025-03-18T23:06:12 Tianqiao Chen: å¯è§AIæ—¶ä»£æ”¾å¼ƒä¼ ç»Ÿæ€è·¯æ˜¯éå¸¸å¿…è¦çš„
[msg_id: 8] 2025-03-18T23:06:30 Tianqiao Chen: çœ‹çœ‹è¿˜æœ‰ä»€ä¹ˆå¯ä»¥é©æ–°çš„
[msg_id: 9] 2025-03-18T23:06:48 Tianqiao Chen: æœç´¢å°±æ˜¯ä¸‹ä¸€ä¸ªäº†
[msg_id: 10] 2025-03-18T23:06:58 Hua ZHANG: æ˜¯çš„ï¼Œè€å¿ƒæµ‹è¯•å¯¹æ¯”ï¼Œè¿˜æ˜¯èƒ½å®ç°æˆ‘ä»¬è¦çš„æ•ˆæœğŸ˜„
[msg_id: 11] 2025-03-18T23:25:15 Tianqiao Chen: å¦‚æœèƒ½æŠŠå—¯ï¼Œå•Šè¿™äº›å£è¯­åŒ–å»æ‰å°±æ›´å¥½äº†ï¼Œå°±åƒæˆ‘è¯´smart replyç”¨è¯­éŸ³ä¿®æ”¹ä¸€æ ·
[msg_id: 12] 2025-03-18T23:25:26 Tianqiao Chen: ä¸è¿‡å¯èƒ½ä¸å¤Ÿå‡†ç¡®ğŸ˜€
[msg_id: 13] 2025-03-18T23:28:32 Hua ZHANG: å½“å‰ç‰¹æ„è½¬çš„å’ŒåŸå§‹è¯­éŸ³å†…å®¹ä¸€æ‘¸ä¸€æ ·ã€‚æˆ‘ä»¬å¯ä»¥è°ƒæ•´promptè¯•è¯•ä¿®é¥°æ•ˆæœã€‚ä½†æ˜¯ä¿®é¥°äº†ä¼šä¸ä¼šå°±æ”¹å˜äº†åŠŸèƒ½ç‰¹ç‚¹äº†ï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿå¯ä»¥åŠ ä¸Šç¿»è¯‘ï¼Œè®©AIç›´æ¥è½¬æˆå…¶ä»–è¯­ç§
[msg_id: 14] 2025-03-18T23:29:08 Tianqiao Chen: æ˜¯çš„
[msg_id: 15] 2025-03-18T23:29:32 Tianqiao Chen: è¯­éŸ³è¿™è¾¹å¯ä»¥åšçš„åŠŸèƒ½æŒºå¤šçš„ï¼Œä½ ä»¬æŠ€æœ¯éƒ¨é—¨æƒ³æƒ³æ€ä¹ˆæ ·åšæˆä¸€ä¸ªäº®ç‚¹åŠŸèƒ½
[msg_id: 16] 2025-03-18T23:30:15 Hua ZHANG: å¥½çš„ï¼Œæˆ‘ä»¬å¤šè¯•è¯•ä¸åŒæ•ˆæœçœ‹çœ‹

Instructions: Analyze the above text, explain your reasoning inside <think> tags, and output a structured knowledge graph in JSON format inside <answer> tags. The JSON should have two keys: 'nodes' and 'edges'. For example:
{"nodes": [{"id": 1, "label": "EntityName", "type": "EntityType"}, ...],"edges": [{"source": 1, "target": 2, "relation": "RELATION_TYPE", "description": "Explanation of the relationship", "keywords": ["key1", "key2","key3"], "strength": 8, "msg_ids": [0, 1, 2]}, ...]}
Requirements:
- Ensure your output strictly follows this format.
- For each relationship/edge, include:
    - source: ID of the source entity
    - target: ID of the target entity
    - relation: Only one type of relationship for each edge
    - description: Detailed explanation of why the entities are related
    - keywords: List of key terms that can be searched or represent important elements of the relationship
    - strength: Numeric score (1-10) indicating relationship strength
    - msg_ids: List of message IDs where this relationship was mentioned
- The output language is English. For specific entities (person name, product name), use the the original language of in the text.
- msg_ids can not be empty.""",
    ]
    
    results = []
    
    for i, prompt in enumerate(prompts):
        print(f"\n\næµ‹è¯• {i+1}: {'å¼•å¯¼ä½¿ç”¨thinkæ ‡ç­¾' if '<think>' in prompt else 'ä¸å¼•å¯¼ä½¿ç”¨thinkæ ‡ç­¾'}")
        print("-" * 50)
        
        # ç”Ÿæˆå›ç­”
        print("æ­£åœ¨è¯·æ±‚LLMç”Ÿæˆå›ç­”...")
        start_time = time.time()
        response = call_vllm_qwen(prompt)
        duration = time.time() - start_time
        print(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        
        # å°è¯•æå–æ€è€ƒå†…å®¹
        thinking = extract_thinking_content(response)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†thinkæ ‡ç­¾
        has_think_tag = "<think>" in response.lower() and "</think>" in response.lower()
        
        result = {
            "prompt_type": "å¼•å¯¼ä½¿ç”¨thinkæ ‡ç­¾" if "<think>" in prompt else "ä¸å¼•å¯¼ä½¿ç”¨thinkæ ‡ç­¾",
            "response_length": len(response),
            "has_think_tag": has_think_tag,
            "thinking_length": len(thinking),
            "thinking_extracted": bool(thinking),
        }
        
        results.append(result)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\nç»“æœæ‘˜è¦:")
        print(f"å›ç­”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"æ˜¯å¦åŒ…å«thinkæ ‡ç­¾: {'æ˜¯' if has_think_tag else 'å¦'}")
        print(f"æå–åˆ°çš„æ€è€ƒå†…å®¹é•¿åº¦: {len(thinking)} å­—ç¬¦")
        print(f"æ˜¯å¦æˆåŠŸæå–æ€è€ƒå†…å®¹: {'æ˜¯' if thinking else 'å¦'}")
        
        # æ‰“å°å›ç­”çš„å‰100ä¸ªå­—ç¬¦
        print("\nå›ç­”æ‘˜è¦ (å‰100ä¸ªå­—ç¬¦):")
        print(response[:100] + "..." if len(response) > 100 else response)
        
        # å¦‚æœæå–åˆ°æ€è€ƒå†…å®¹ï¼Œæ‰“å°å‰100ä¸ªå­—ç¬¦
        if thinking:
            print("\næå–åˆ°çš„æ€è€ƒå†…å®¹ (å‰100ä¸ªå­—ç¬¦):")
            print(thinking[:100] + "..." if len(thinking) > 100 else thinking)
        
        print("-" * 50)
    
    # æ‰“å°æ€»ç»“
    print("\n\næµ‹è¯•æ€»ç»“:")
    print("-" * 50)
    for i, result in enumerate(results):
        print(f"æµ‹è¯• {i+1} ({result['prompt_type']}):")
        print(f"  å›ç­”é•¿åº¦: {result['response_length']} å­—ç¬¦")
        print(f"  æ˜¯å¦åŒ…å«thinkæ ‡ç­¾: {'æ˜¯' if result['has_think_tag'] else 'å¦'}")
        print(f"  æå–åˆ°çš„æ€è€ƒå†…å®¹é•¿åº¦: {result['thinking_length']} å­—ç¬¦")
        print(f"  æ˜¯å¦æˆåŠŸæå–æ€è€ƒå†…å®¹: {'æ˜¯' if result['thinking_extracted'] else 'å¦'}")
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    try:
        with open("llm_thinking_extraction_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print("\nç»“æœå·²ä¿å­˜åˆ° llm_thinking_extraction_results.json")
    except Exception as e:
        print(f"\nä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    test_think_tag_generation() 