{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638e2da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行完整数据转换...\n",
      "训练文件数量: 1\n",
      "验证文件数量: 1\n",
      "去重选项: 启用\n",
      "--------------------------------------------------\n",
      "正在验证文件...\n",
      "✓ 文件验证通过: /tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_train.json\n",
      "✓ 文件验证通过: /tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_val.json\n",
      "正在读取训练数据: /tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_train.json\n",
      "  读取到 13428 条数据\n",
      "正在读取验证数据: /tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_val.json\n",
      "  读取到 1492 条数据\n",
      "\n",
      "合并后数据统计:\n",
      "训练数据: 13428 条 (来自 1 个文件)\n",
      "验证数据: 1492 条 (来自 1 个文件)\n",
      "\n",
      "正在进行数据去重...\n",
      "训练数据去重: 13428 -> 13428 (移除 0 条)\n",
      "验证数据去重: 1492 -> 1492 (移除 0 条)\n",
      "正在转换训练数据...\n",
      "正在转换验证数据...\n",
      "正在保存为parquet格式...\n",
      "训练数据已保存至: /tcci_mnt/shihao/project/verl/psy_r1/SMHC_data_v4/train.parquet\n",
      "验证数据已保存至: /tcci_mnt/shihao/project/verl/psy_r1/SMHC_data_v4/val.parquet\n",
      "训练数据量: 13428 条\n",
      "验证数据量: 1492 条\n",
      "\n",
      "样例数据 (训练集第一条):\n",
      "{\n",
      "  \"data_source\": \"SMHC_SFT_auxiliary_diagnosis\",\n",
      "  \"prompt\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"你是一位经验丰富的精神科医生。请阅读以下病人来精神科问诊的对话记录，并判断病人的主要心理健康状况。抑郁症以**持续的情绪低落、兴趣减退和精力缺乏**为主，而焦虑症则以**过度担忧、紧张不安和对未来事件的恐惧**为主要特点。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"\\n[问诊对话开始]\\n未知发言人：最近感觉怎么样？情绪上有什么特别的变化吗？\\n未知发言人：最近感觉特别糟糕，每天都提不起精神，做什么事情都没有兴趣，脑子里总是空荡荡的。有时候会突然觉得很绝望，觉得活着没什么意义。反应也变慢了，说话的时候经常想半天才能组织好语言。情绪一直很低落，大部分时间都高兴不起来，觉得自己毫无价值，常常自责和内疚。睡眠问题也很严重，入睡困难、早醒、睡得浅，整夜整夜地翻来覆去。虽然胃纳还行，但身体总是觉得很疲劳、没力气。这些症状已经持续了五个月了，最近一周因为家里的一些事变得更严重了。\\n未知发言人：最近有没有出现不想吃饭或者突然特别想吃很多东西的情况？\\n未知发言人：没有，我最近的胃口还算正常，没有特别不想吃饭或者暴饮暴食的情况。\\n未知发言人：你最近是不是发现自己做事容易分心或者记不住事情？比如看书的时候老是走神或者刚说过的话转眼就忘了？\\n未知发言人：是的，我最近注意力特别差，看书的时候经常会走神，脑子里乱七八糟的念头不断出现，根本没办法集中精力。记忆力也明显变差了，有时候刚做完的事情转眼就忘了，比如刚才放在哪里的东西一会儿就找不到了。学习新东西变得特别困难，即使以前掌握过的一些技能现在也很难再熟练运用。这种状态让我很焦虑，感觉自己越来越跟不上节奏了。\\n未知发言人：你提到情绪一直很低落觉得自己毫无价值这种状态下有没有出现想伤害自己或者结束生命的想法？\\n未知发言人：有，我经常有这样的想法，觉得活着没有意义，不如死了算了。有时候会计划一些方式，甚至尝试过自残。这些想法和行为让我很害怕，但又控制不住自己。\\n未知发言人：你爸妈知道你现在这些情况吗有没有跟他们沟通过最近的状况\\n未知发言人：爸妈知道一些情况，但可能他们并不完全了解我现在到底有多糟糕。我跟他们沟通过，但每次说完之后他们总是用一些让我觉得不被理解的方式回应我，比如他们会说“你年轻，过段时间就好了”或者“别想太多”。这些话让我觉得很无助，好像我的痛苦在他们眼里根本不重要。其实我也不想让他们担心，所以很多时候我都选择自己默默承受。\\n未知发言人：最近的学习或者工作任务有没有因为这些情况变得更难应对了\\n未知发言人：是的，我的学习状态真的很糟糕，上课的时候经常走神，老师讲的内容我听不进去，笔记也记不全。作业对我来说变得特别困难，以前能轻松完成的题目现在要花很久才能做出来，甚至有时候完全不知道从哪里下手。考试成绩也越来越差，这让我更加自责和焦虑。我感觉自己在退步，在被别人甩得越来越远。有时候坐在教室里看着同学们认真做题的样子，我会觉得特别孤独和无助。虽然我也想努力跟上大家的节奏但真的力不从心。\\n未知发言人：最近晚上睡得怎么样是很难入睡还是容易醒\\n未知发言人：最近晚上睡得特别差，入睡变得很困难，有时候躺到床上很久都睡不着脑子里一直在想一些乱七八糟的事情。好不容易睡着了也总是浅浅的，稍微有点动静就会醒过来。最严重的是早醒，天还没亮我就醒了然后就再也睡不着了。这样的情况已经持续一段时间了每天早上起来都感觉特别累但又没真正休息好整个人的状态都很差。\\n未知发言人：你平时有没有觉得胸口发闷或者呼吸不畅的情况这种感觉是经常性的还是偶尔出现\\n未知发言人：没有，我平时没有觉得胸口发闷或者呼吸不畅的情况。\\n未知发言人：你最近有没有觉得脑子里总有一些声音在说话 这些声音是真实的还是想象出来的\\n未知发言人：没有，我脑子里没有出现过声音在说话的情况。\\n未知发言人：最近有没有觉得心跳突然快起来或者感觉心脏扑通扑通跳得厉害 这种情况是经常发生还是偶尔\\n未知发言人：没有，我最近没有觉得心跳突然加快或者特别明显地跳得厉害的情况。\\n未知发言人：你平时月经周期规律吗有没有出现过经期前后情绪特别低落或者烦躁的情况\\n未知发言人：我的月经周期一直还算规律，大概每个月都会按时来没有出现过特别大的问题。不过经期前后有时候会感觉情绪不太稳定，但这种情况不是特别严重也没有特别明显的感觉。最近几个月好像没有因为经期而感到特别低落或者烦躁的情况。\\n未知发言人：你平时有没有经常感觉头晕或者有头痛的情况 这些情况是偶尔出现还是经常发生\\n未知发言人：我最近偶尔会有点头晕的情况，特别是在晚上睡不好觉之后第二天起床的时候会感觉头有点晕晕的，但不是特别严重。至于头痛的话，倒是没有经常出现这种情况，偶尔会觉得脑袋里闷闷的，像是有压力压着一样，但也不是持续性的。这种感觉一般过一会儿就会缓解。\\n未知发言人：你最近是不是有时候会觉得做什么都没劲儿 对未来也没什么期待 这种感觉多不多\\n未知发言人：是的，我经常觉得做什么都没劲儿对未来也没什么期待。我经常会感到悲观绝望，觉得自己毫无价值，常常会因为这种想法而哭泣流泪。这种感觉几乎每天都有，有时候甚至一整天都高兴不起来。\\n未知发言人：你有没有过突然特别兴奋或者精力旺盛的时候？比如睡眠少但很精神做事特别快或者话多\\n未知发言人：没有，我从来没有过突然特别兴奋或者精力旺盛的时候也没有出现过睡眠少但很精神做事特别快或者话多的情况。我的状态一直是低落的，连基本的精力都很难维持。\\n未知发言人：你觉得别人通常怎么评价你的性格\\n未知发言人：我觉得以前的我性格比较外向，也比较急躁，喜欢和人交流，遇到事情会比较直接地表达出来。但现在的话，我变得很内向，不太愿意和别人接触，也不太想说话。有时候别人跟我说话我也会回应，但内心其实很抗拒。我现在感觉自己整个人都变了，变得消极、自卑、没有以前那种活力了。\\n未知发言人：你现在有在恋爱吗或者最近跟男朋友女朋友相处得怎么样会不会因为感情的事情让你更难过\\n未知发言人：我未婚，也没有恋爱史。最近也没有开始任何新的感情关系，所以不存在和男朋友或者女朋友相处的问题。感情方面对我来说已经很久没有概念了，现在的我连基本的社交都变得困难，更别说谈恋爱了。\\n未知发言人：你家里人有没有得过抑郁症或者别的精神方面疾病的比如父母兄弟姐妹或者亲戚\\n[问诊对话结束]\\n\\n请从以下四个选项中选择最合适的诊断：\\n- 抑郁：主要表现为抑郁症状, 满足ICD诊断要求。 \\n- 焦虑：主要表现为焦虑症状, 满足ICD诊断要求。 \\n- mix：同时表现出明显的抑郁和焦虑症状均满足ICD诊断要求，或者都没有满足单诊断为抑郁和焦虑的程度。\\n- others：其他心理健康问题（比如双向情感障碍，精神分裂症，等等）或正常状态。\\n\\n请一步一步思考，将思考过程放在<think></think>标签中，最后再将结果（\\\"抑郁\\\"、\\\"焦虑\\\"、\\\"mix\\\"或\\\"others\\\"）放在<box></box>中输出。\"\n",
      "    }\n",
      "  ],\n",
      "  \"ability\": \"medical_diagnosis\",\n",
      "  \"reward_model\": {\n",
      "    \"style\": \"rule\",\n",
      "    \"ground_truth\": \"抑郁\"\n",
      "  },\n",
      "  \"extra_info\": {\n",
      "    \"split\": \"train\",\n",
      "    \"index\": 0,\n",
      "    \"patient_id\": \"337_conv5\",\n",
      "    \"original_response\": \"<box>抑郁</box>\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# 提取诊断结果的函数\n",
    "def extract_diagnosis_result(diagnosis_text: str) -> str:\n",
    "    \"\"\"\n",
    "    从诊断文本中提取<box></box>中的诊断结果\n",
    "    \"\"\"\n",
    "    # 查找 <box>xxx</box> 格式的诊断结果\n",
    "    pattern = r'<box>([^<]+)</box>'\n",
    "    match = re.search(pattern, diagnosis_text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"others\"  # 如果没有找到，返回默认值\n",
    "\n",
    "# ICD代码到新标签的映射函数\n",
    "def map_icd_to_new_label(icd_code: str) -> str:\n",
    "    \"\"\"\n",
    "    将原来的ICD代码映射到新的诊断标签\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"F32\": \"抑郁\",\n",
    "        \"F41\": \"焦虑\", \n",
    "        \"F32,F41\": \"mix\",\n",
    "        \"Others\": \"others\"\n",
    "    }\n",
    "    return mapping.get(icd_code, \"others\")\n",
    "\n",
    "# 从原始回答中提取ICD代码并映射到新标签\n",
    "def extract_and_map_ground_truth(gpt_content: str) -> str:\n",
    "    \"\"\"\n",
    "    从GPT回答中提取ICD代码并映射到新的诊断标签\n",
    "    如果回答中包含新格式的<box>标签，直接提取；否则从ICD代码映射\n",
    "    \"\"\"\n",
    "    # 首先尝试提取新格式的<box>标签\n",
    "    box_result = extract_diagnosis_result(gpt_content)\n",
    "    if box_result != \"others\":  # 如果找到了有效的box标签\n",
    "        return box_result\n",
    "    \n",
    "    # 如果没有找到box标签，尝试从ICD代码映射\n",
    "    icd_pattern = r'\\\\icd_code\\{([^}]+)\\}'\n",
    "    icd_match = re.search(icd_pattern, gpt_content)\n",
    "    if icd_match:\n",
    "        icd_code = icd_match.group(1)\n",
    "        return map_icd_to_new_label(icd_code)\n",
    "    \n",
    "    return \"others\"\n",
    "\n",
    "# 数据转换函数\n",
    "def make_map_fn(data_source: str, split: str):\n",
    "    \"\"\"\n",
    "    创建数据转换函数\n",
    "    \"\"\"\n",
    "    def process_fn(data_item: Dict[str, Any], idx: int) -> Dict[str, Any]:\n",
    "        patient_id = data_item['patient_id']\n",
    "        conversations = data_item['conversations']\n",
    "        \n",
    "        # 新的system和user prompt\n",
    "        system_prompt = r\"\"\"你是一位经验丰富的精神科医生。请阅读以下病人来精神科问诊的对话记录，并判断病人的主要心理健康状况。抑郁症以**持续的情绪低落、兴趣减退和精力缺乏**为主，而焦虑症则以**过度担忧、紧张不安和对未来事件的恐惧**为主要特点。\"\"\"\n",
    "        \n",
    "        human_content = \"\"\n",
    "        gpt_content = \"\"\n",
    "        \n",
    "        for conv in conversations:\n",
    "            if conv['from'] == 'human':\n",
    "                human_content = conv['value']\n",
    "            elif conv['from'] == 'gpt':\n",
    "                gpt_content = conv['value']\n",
    "        \n",
    "        # 提取对话文本（去掉原有的prompt前缀）\n",
    "        text = human_content\n",
    "        # 提取[问诊对话开始]到[问诊对话结束]之间的内容\n",
    "        start_marker = \"[问诊对话开始]\"\n",
    "        end_marker = \"[问诊对话结束]\"\n",
    "        start_idx = text.find(start_marker)\n",
    "        end_idx = text.find(end_marker)\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            text = text[start_idx + len(start_marker):end_idx].strip()\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "[问诊对话开始]\n",
    "{text}\n",
    "[问诊对话结束]\n",
    "\n",
    "请从以下四个选项中选择最合适的诊断：\n",
    "- 抑郁：主要表现为抑郁症状, 满足ICD诊断要求。 \n",
    "- 焦虑：主要表现为焦虑症状, 满足ICD诊断要求。 \n",
    "- mix：同时表现出明显的抑郁和焦虑症状均满足ICD诊断要求，或者都没有满足单诊断为抑郁和焦虑的程度。\n",
    "- others：其他心理健康问题（比如双向情感障碍，精神分裂症，等等）或正常状态。\n",
    "\n",
    "请一步一步思考，将思考过程放在<think></think>标签中，最后再将结果（\"抑郁\"、\"焦虑\"、\"mix\"或\"others\"）放在<box></box>中输出。\"\"\"\n",
    "        \n",
    "        # 构建prompt - 按照HuggingFace chat template格式\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # 提取ground truth（映射到新标签）\n",
    "        ground_truth = extract_and_map_ground_truth(gpt_content)\n",
    "        \n",
    "        # 构建转换后的数据\n",
    "        converted_data = {\n",
    "            \"data_source\": data_source,\n",
    "            \"prompt\": prompt,\n",
    "            \"ability\": \"medical_diagnosis\",  # 医疗诊断能力\n",
    "            \"reward_model\": {\n",
    "                \"style\": \"rule\",\n",
    "                \"ground_truth\": ground_truth\n",
    "            },\n",
    "            \"extra_info\": {\n",
    "                \"split\": split,\n",
    "                \"index\": idx,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"original_response\": gpt_content\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return converted_data\n",
    "    \n",
    "    return process_fn\n",
    "\n",
    "def validate_files(files: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    验证文件列表，移除不存在的文件\n",
    "    \"\"\"\n",
    "    valid_files = []\n",
    "    for file_path in files:\n",
    "        if os.path.exists(file_path):\n",
    "            valid_files.append(file_path)\n",
    "            print(f\"✓ 文件验证通过: {file_path}\")\n",
    "        else:\n",
    "            print(f\"✗ 文件不存在，已跳过: {file_path}\")\n",
    "    return valid_files\n",
    "\n",
    "def remove_duplicates_by_patient_id(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    根据patient_id去除重复数据，保留第一次出现的数据\n",
    "    \"\"\"\n",
    "    seen_ids = set()\n",
    "    deduplicated_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        patient_id = item.get('patient_id', '')\n",
    "        if patient_id not in seen_ids:\n",
    "            seen_ids.add(patient_id)\n",
    "            deduplicated_data.append(item)\n",
    "        else:\n",
    "            print(f\"发现重复数据，已跳过: {patient_id}\")\n",
    "    \n",
    "    return deduplicated_data\n",
    "\n",
    "def convert_sharegpt_to_parquet(train_files: List[str], val_files: List[str], output_dir: str, \n",
    "                               remove_duplicates: bool = True):\n",
    "    \"\"\"\n",
    "    将ShareGPT格式的数据转换为后训练格式的parquet文件\n",
    "    支持多个训练文件和验证文件输入\n",
    "    \n",
    "    Args:\n",
    "        train_files: 训练数据文件路径列表\n",
    "        val_files: 验证数据文件路径列表 \n",
    "        output_dir: 输出目录\n",
    "        remove_duplicates: 是否根据patient_id去除重复数据\n",
    "    \"\"\"\n",
    "    data_source = \"SMHC_SFT_auxiliary_diagnosis\"\n",
    "    \n",
    "    # 验证文件存在性\n",
    "    print(\"正在验证文件...\")\n",
    "    train_files = validate_files(train_files)\n",
    "    val_files = validate_files(val_files)\n",
    "    \n",
    "    if not train_files:\n",
    "        raise ValueError(\"没有找到有效的训练文件\")\n",
    "    if not val_files:\n",
    "        raise ValueError(\"没有找到有效的验证文件\")\n",
    "    \n",
    "    # 读取多个训练数据文件\n",
    "    train_data = []\n",
    "    total_train_items = 0\n",
    "    for train_file in train_files:\n",
    "        print(f\"正在读取训练数据: {train_file}\")\n",
    "        with open(train_file, 'r', encoding='utf-8') as f:\n",
    "            file_data = json.load(f)\n",
    "            train_data.extend(file_data)\n",
    "            total_train_items += len(file_data)\n",
    "            print(f\"  读取到 {len(file_data)} 条数据\")\n",
    "    \n",
    "    # 读取多个验证数据文件\n",
    "    val_data = []\n",
    "    total_val_items = 0\n",
    "    for val_file in val_files:\n",
    "        print(f\"正在读取验证数据: {val_file}\")\n",
    "        with open(val_file, 'r', encoding='utf-8') as f:\n",
    "            file_data = json.load(f)\n",
    "            val_data.extend(file_data)\n",
    "            total_val_items += len(file_data)\n",
    "            print(f\"  读取到 {len(file_data)} 条数据\")\n",
    "    \n",
    "    print(f\"\\n合并后数据统计:\")\n",
    "    print(f\"训练数据: {total_train_items} 条 (来自 {len(train_files)} 个文件)\")\n",
    "    print(f\"验证数据: {total_val_items} 条 (来自 {len(val_files)} 个文件)\")\n",
    "    \n",
    "    # 去重处理\n",
    "    if remove_duplicates:\n",
    "        print(\"\\n正在进行数据去重...\")\n",
    "        original_train_count = len(train_data)\n",
    "        original_val_count = len(val_data)\n",
    "        \n",
    "        train_data = remove_duplicates_by_patient_id(train_data)\n",
    "        val_data = remove_duplicates_by_patient_id(val_data)\n",
    "        \n",
    "        print(f\"训练数据去重: {original_train_count} -> {len(train_data)} (移除 {original_train_count - len(train_data)} 条)\")\n",
    "        print(f\"验证数据去重: {original_val_count} -> {len(val_data)} (移除 {original_val_count - len(val_data)} 条)\")\n",
    "    \n",
    "    # 创建转换函数\n",
    "    train_map_fn = make_map_fn(data_source, 'train')\n",
    "    val_map_fn = make_map_fn(data_source, 'val')\n",
    "    \n",
    "    # 转换数据\n",
    "    print(\"正在转换训练数据...\")\n",
    "    converted_train = [train_map_fn(item, idx) for idx, item in enumerate(train_data)]\n",
    "    \n",
    "    print(\"正在转换验证数据...\")\n",
    "    converted_val = [val_map_fn(item, idx) for idx, item in enumerate(val_data)]\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 转换为DataFrame并保存为parquet\n",
    "    print(\"正在保存为parquet格式...\")\n",
    "    train_df = pd.DataFrame(converted_train)\n",
    "    val_df = pd.DataFrame(converted_val)\n",
    "    \n",
    "    train_output_path = os.path.join(output_dir, 'train.parquet')\n",
    "    val_output_path = os.path.join(output_dir, 'val.parquet')\n",
    "    \n",
    "    train_df.to_parquet(train_output_path, index=False)\n",
    "    val_df.to_parquet(val_output_path, index=False)\n",
    "    \n",
    "    print(f\"训练数据已保存至: {train_output_path}\")\n",
    "    print(f\"验证数据已保存至: {val_output_path}\")\n",
    "    print(f\"训练数据量: {len(converted_train)} 条\")\n",
    "    print(f\"验证数据量: {len(converted_val)} 条\")\n",
    "    \n",
    "    # 显示样例数据\n",
    "    print(\"\\n样例数据 (训练集第一条):\")\n",
    "    print(str(json.dumps(converted_train[0], ensure_ascii=False, indent=2)))\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def add_multiple_files_example():\n",
    "    \"\"\"\n",
    "    示例：如何添加多个文件\n",
    "    \"\"\"\n",
    "    # 方式1：直接列出所有文件\n",
    "    train_files = [\n",
    "        \"/tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_train.json\",\n",
    "        # \"/path/to/additional_train_file1.json\",\n",
    "        # \"/path/to/additional_train_file2.json\",\n",
    "    ]\n",
    "    \n",
    "    val_files = [\n",
    "        \"/tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_val.json\",\n",
    "        # \"/path/to/additional_val_file1.json\", \n",
    "        # \"/path/to/additional_val_file2.json\",\n",
    "    ]\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "def get_files_from_pattern(pattern: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    根据文件模式获取文件列表\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    files = glob.glob(pattern)\n",
    "    files.sort()  # 确保文件顺序一致\n",
    "    return files\n",
    "\n",
    "# 设置文件路径 - 支持多文件输入\n",
    "train_files, val_files = add_multiple_files_example()\n",
    "\n",
    "# 你也可以使用文件模式匹配（如果有多个相似命名的文件）\n",
    "# train_files = get_files_from_pattern(\"/path/to/train_data_*.json\")\n",
    "# val_files = get_files_from_pattern(\"/path/to/val_data_*.json\")\n",
    "\n",
    "output_dir = \"/tcci_mnt/shihao/project/verl/psy_r1/SMHC_data_v4\"\n",
    "\n",
    "# 配置选项\n",
    "REMOVE_DUPLICATES = True  # 是否去除重复数据\n",
    "\n",
    "# 执行转换\n",
    "print(\"开始执行完整数据转换...\")\n",
    "print(f\"训练文件数量: {len(train_files)}\")\n",
    "print(f\"验证文件数量: {len(val_files)}\")\n",
    "print(f\"去重选项: {'启用' if REMOVE_DUPLICATES else '禁用'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_df, val_df = convert_sharegpt_to_parquet(\n",
    "    train_files=train_files, \n",
    "    val_files=val_files, \n",
    "    output_dir=output_dir,\n",
    "    remove_duplicates=REMOVE_DUPLICATES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19646ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据量: 13428\n",
      "验证集数据量: 1492\n",
      "\n",
      "数据列名:\n",
      "['data_source', 'prompt', 'ability', 'reward_model', 'extra_info']\n",
      "\n",
      "数据样例:\n",
      "第一条数据的结构:\n",
      "- data_source: SMHC_SFT_auxiliary_diagnosis\n",
      "- ability: medical_diagnosis\n",
      "- prompt: 包含 2 个消息\n",
      "- reward_model: {'ground_truth': '抑郁', 'style': 'rule'}\n",
      "- extra_info: {'index': 0, 'original_response': '<box>抑郁</box>', 'patient_id': '337_conv5', 'split': 'train'}\n",
      "\n",
      "- prompt内容预览:\n",
      "  角色: system\n",
      "  内容: 你是一位经验丰富的精神科医生。请阅读以下病人来精神科问诊的对话记录，并判断病人的主要心理健康状况。抑郁症以**持续的情绪低落、兴趣减退和精力缺乏**为主，而焦虑症则以**过度担忧、紧张不安和对未来事件的恐惧**为主要特点。\n",
      "\n",
      "诊断类别分布:\n",
      "训练集:\n",
      "reward_model\n",
      "抑郁        5787\n",
      "others    3465\n",
      "mix       3096\n",
      "焦虑        1080\n",
      "Name: count, dtype: int64\n",
      "\n",
      "验证集:\n",
      "reward_model\n",
      "抑郁        643\n",
      "others    385\n",
      "mix       344\n",
      "焦虑        120\n",
      "Name: count, dtype: int64\n",
      "\n",
      "数据质量检查:\n",
      "训练集空值检查: 0\n",
      "验证集空值检查: 0\n",
      "\n",
      "后训练数据使用示例:\n",
      "每条数据包含以下字段:\n",
      "- data_source: 数据来源\n",
      "- prompt: HuggingFace chat template格式的对话\n",
      "- ability: 任务类型 (medical_diagnosis)\n",
      "- reward_model: 包含ground_truth的奖励模型信息\n",
      "- extra_info: 额外信息 (split, index, patient_id, original_response)\n",
      "\n",
      "转换完成！数据已准备好用于后训练。\n"
     ]
    }
   ],
   "source": [
    "# 验证和使用生成的parquet文件\n",
    "\n",
    "# 1. 读取parquet文件\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 读取生成的parquet文件\n",
    "train_df = pd.read_parquet('/tcci_mnt/shihao/project/verl/psy_r1/SMHC_data_v4/train.parquet')\n",
    "val_df = pd.read_parquet('/tcci_mnt/shihao/project/verl/psy_r1/SMHC_data_v4/val.parquet')\n",
    "\n",
    "print(f\"训练集数据量: {len(train_df)}\")\n",
    "print(f\"验证集数据量: {len(val_df)}\")\n",
    "print(\"\\n数据列名:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# 2. 检查数据结构\n",
    "print(\"\\n数据样例:\")\n",
    "sample = train_df.iloc[0]\n",
    "\n",
    "# 安全地显示样例数据，避免numpy数组序列化问题\n",
    "print(\"第一条数据的结构:\")\n",
    "print(f\"- data_source: {sample['data_source']}\")\n",
    "print(f\"- ability: {sample['ability']}\")\n",
    "print(f\"- prompt: 包含 {len(sample['prompt'])} 个消息\")\n",
    "print(f\"- reward_model: {sample['reward_model']}\")\n",
    "print(f\"- extra_info: {sample['extra_info']}\")\n",
    "\n",
    "# 显示prompt的内容（截取前200个字符）\n",
    "print(f\"\\n- prompt内容预览:\")\n",
    "prompt_content = sample['prompt'][0]['content'][:200] + \"...\" if len(sample['prompt'][0]['content']) > 200 else sample['prompt'][0]['content']\n",
    "print(f\"  角色: {sample['prompt'][0]['role']}\")\n",
    "print(f\"  内容: {prompt_content}\")\n",
    "\n",
    "# 3. 统计不同诊断类别的分布\n",
    "print(\"\\n诊断类别分布:\")\n",
    "\n",
    "# 训练集诊断分布\n",
    "train_diagnosis = train_df['reward_model'].apply(lambda x: x['ground_truth'])\n",
    "print(\"训练集:\")\n",
    "print(train_diagnosis.value_counts())\n",
    "\n",
    "# 验证集诊断分布\n",
    "val_diagnosis = val_df['reward_model'].apply(lambda x: x['ground_truth'])\n",
    "print(\"\\n验证集:\")\n",
    "print(val_diagnosis.value_counts())\n",
    "\n",
    "# 4. 检查数据质量\n",
    "print(\"\\n数据质量检查:\")\n",
    "print(f\"训练集空值检查: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"验证集空值检查: {val_df.isnull().sum().sum()}\")\n",
    "\n",
    "# 5. 示例：如何使用数据进行后训练\n",
    "print(\"\\n后训练数据使用示例:\")\n",
    "print(\"每条数据包含以下字段:\")\n",
    "print(\"- data_source: 数据来源\")\n",
    "print(\"- prompt: HuggingFace chat template格式的对话\")\n",
    "print(\"- ability: 任务类型 (medical_diagnosis)\")\n",
    "print(\"- reward_model: 包含ground_truth的奖励模型信息\")\n",
    "print(\"- extra_info: 额外信息 (split, index, patient_id, original_response)\")\n",
    "\n",
    "print(\"\\n转换完成！数据已准备好用于后训练。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267781d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 多文件输入功能演示 ===\n",
      "\n",
      "使用方式：\n",
      "1. 手动指定文件列表:\n",
      "   train_files, val_files = example_manual_files()\n",
      "\n",
      "2. 使用模式匹配:\n",
      "   train_files, val_files = example_pattern_matching()\n",
      "\n",
      "3. 扫描目录:\n",
      "   train_files, val_files = example_directory_scan('/train/dir', '/val/dir')\n",
      "\n",
      "4. 交互式选择:\n",
      "   train_files, val_files = example_interactive_selection()\n",
      "\n",
      "然后调用:\n",
      "   convert_sharegpt_to_parquet(train_files, val_files, output_dir)\n",
      "\n",
      "当前配置的文件:\n",
      "训练文件 (1):\n",
      "  1. /tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_train.json\n",
      "\n",
      "验证文件 (1):\n",
      "  1. /tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_val.json\n",
      "\n",
      "总计: 1 个训练文件, 1 个验证文件\n"
     ]
    }
   ],
   "source": [
    "# 多文件输入的使用示例\n",
    "\n",
    "print(\"=== 多文件输入功能演示 ===\\n\")\n",
    "\n",
    "# 示例1：手动指定多个文件\n",
    "def example_manual_files():\n",
    "    \"\"\"手动指定多个JSON文件\"\"\"\n",
    "    train_files = [\n",
    "        \"/path/to/dataset1_train.json\",\n",
    "        \"/path/to/dataset2_train.json\", \n",
    "        \"/path/to/dataset3_train.json\",\n",
    "    ]\n",
    "    \n",
    "    val_files = [\n",
    "        \"/path/to/dataset1_val.json\",\n",
    "        \"/path/to/dataset2_val.json\",\n",
    "        \"/path/to/dataset3_val.json\", \n",
    "    ]\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "# 示例2：使用文件模式匹配\n",
    "def example_pattern_matching():\n",
    "    \"\"\"使用glob模式匹配文件\"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # 匹配所有以特定模式命名的训练文件\n",
    "    train_files = get_files_from_pattern(\"/path/to/*_train.json\")\n",
    "    val_files = get_files_from_pattern(\"/path/to/*_val.json\")\n",
    "    \n",
    "    # 或者匹配特定目录下的所有JSON文件\n",
    "    # train_files = get_files_from_pattern(\"/path/to/train_data/*.json\")\n",
    "    # val_files = get_files_from_pattern(\"/path/to/val_data/*.json\")\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "# 示例3：从目录中读取所有JSON文件\n",
    "def example_directory_scan(train_dir: str, val_dir: str):\n",
    "    \"\"\"扫描目录中的所有JSON文件\"\"\"\n",
    "    import os\n",
    "    \n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    # 扫描训练数据目录\n",
    "    if os.path.exists(train_dir):\n",
    "        for filename in sorted(os.listdir(train_dir)):\n",
    "            if filename.endswith('.json'):\n",
    "                train_files.append(os.path.join(train_dir, filename))\n",
    "    \n",
    "    # 扫描验证数据目录  \n",
    "    if os.path.exists(val_dir):\n",
    "        for filename in sorted(os.listdir(val_dir)):\n",
    "            if filename.endswith('.json'):\n",
    "                val_files.append(os.path.join(val_dir, filename))\n",
    "                \n",
    "    return train_files, val_files\n",
    "\n",
    "# 示例4：交互式文件选择\n",
    "def example_interactive_selection():\n",
    "    \"\"\"交互式添加文件\"\"\"\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    print(\"请输入训练文件路径（输入'done'结束）:\")\n",
    "    while True:\n",
    "        file_path = input(\"训练文件: \").strip()\n",
    "        if file_path.lower() == 'done':\n",
    "            break\n",
    "        if file_path and os.path.exists(file_path):\n",
    "            train_files.append(file_path)\n",
    "            print(f\"✓ 已添加: {file_path}\")\n",
    "        elif file_path:\n",
    "            print(f\"✗ 文件不存在: {file_path}\")\n",
    "    \n",
    "    print(\"\\n请输入验证文件路径（输入'done'结束）:\")\n",
    "    while True:\n",
    "        file_path = input(\"验证文件: \").strip()\n",
    "        if file_path.lower() == 'done':\n",
    "            break\n",
    "        if file_path and os.path.exists(file_path):\n",
    "            val_files.append(file_path)\n",
    "            print(f\"✓ 已添加: {file_path}\")\n",
    "        elif file_path:\n",
    "            print(f\"✗ 文件不存在: {file_path}\")\n",
    "            \n",
    "    return train_files, val_files\n",
    "\n",
    "print(\"使用方式：\")\n",
    "print(\"1. 手动指定文件列表:\")\n",
    "print(\"   train_files, val_files = example_manual_files()\")\n",
    "print()\n",
    "print(\"2. 使用模式匹配:\")\n",
    "print(\"   train_files, val_files = example_pattern_matching()\")\n",
    "print()\n",
    "print(\"3. 扫描目录:\")\n",
    "print(\"   train_files, val_files = example_directory_scan('/train/dir', '/val/dir')\")\n",
    "print()\n",
    "print(\"4. 交互式选择:\")\n",
    "print(\"   train_files, val_files = example_interactive_selection()\")\n",
    "print()\n",
    "print(\"然后调用:\")\n",
    "print(\"   convert_sharegpt_to_parquet(train_files, val_files, output_dir)\")\n",
    "\n",
    "# 检查当前使用的文件\n",
    "print(f\"\\n当前配置的文件:\")\n",
    "print(f\"训练文件 ({len(train_files)}):\")\n",
    "for i, f in enumerate(train_files, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "    \n",
    "print(f\"\\n验证文件 ({len(val_files)}):\")\n",
    "for i, f in enumerate(val_files, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "\n",
    "print(f\"\\n总计: {len(train_files)} 个训练文件, {len(val_files)} 个验证文件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a500670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 多文件数据处理最佳实践 ===\n",
      "\n",
      "使用高级多文件处理:\n",
      "数据源: primary\n",
      "  训练文件: 1 个\n",
      "  验证文件: 1 个\n",
      "\n",
      "总计:\n",
      "  训练文件: 1 个\n",
      "  验证文件: 1 个\n",
      "\n",
      "数据质量检查:\n",
      "训练数据质量报告:\n",
      "  有效文件: 1/1\n",
      "  总记录数: 13428\n",
      "  唯一患者: 13428\n",
      "验证数据质量报告:\n",
      "  有效文件: 1/1\n",
      "  总记录数: 1492\n",
      "  唯一患者: 1492\n",
      "\n",
      "要执行批量处理，取消注释下面的代码:\n",
      "# batch_process_multiple_datasets()\n"
     ]
    }
   ],
   "source": [
    "# 最佳实践示例：多文件数据处理完整流程\n",
    "\n",
    "print(\"=== 多文件数据处理最佳实践 ===\\n\")\n",
    "\n",
    "def advanced_multi_file_processing():\n",
    "    \"\"\"\n",
    "    高级多文件处理示例，包含完整的数据处理流程\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 定义数据源配置\n",
    "    data_sources = {\n",
    "        \"primary\": {\n",
    "            \"train\": [\"/tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_train.json\"],\n",
    "            \"val\": [\"/tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_val.json\"],\n",
    "        },\n",
    "        # 可以添加更多数据源\n",
    "        # \"secondary\": {\n",
    "        #     \"train\": [\"/path/to/secondary_train1.json\", \"/path/to/secondary_train2.json\"],\n",
    "        #     \"val\": [\"/path/to/secondary_val1.json\"],\n",
    "        # },\n",
    "    }\n",
    "    \n",
    "    # 2. 合并所有数据源\n",
    "    all_train_files = []\n",
    "    all_val_files = []\n",
    "    \n",
    "    for source_name, files in data_sources.items():\n",
    "        print(f\"数据源: {source_name}\")\n",
    "        print(f\"  训练文件: {len(files['train'])} 个\")\n",
    "        print(f\"  验证文件: {len(files['val'])} 个\")\n",
    "        \n",
    "        all_train_files.extend(files['train'])\n",
    "        all_val_files.extend(files['val'])\n",
    "    \n",
    "    print(f\"\\n总计:\")\n",
    "    print(f\"  训练文件: {len(all_train_files)} 个\")\n",
    "    print(f\"  验证文件: {len(all_val_files)} 个\")\n",
    "    \n",
    "    return all_train_files, all_val_files\n",
    "\n",
    "def batch_process_multiple_datasets():\n",
    "    \"\"\"\n",
    "    批量处理多个数据集的示例\n",
    "    \"\"\"\n",
    "    datasets = [\n",
    "        {\n",
    "            \"name\": \"SMHC_v4\",\n",
    "            \"train_files\": [\"/tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_train.json\"],\n",
    "            \"val_files\": [\"/tcci_mnt/shihao/project/LLaMA-Factory/shihao/data/SMHC_SFT_auxiliary_diagnosis_training_data_v4_val.json\"],\n",
    "            \"output_dir\": \"/tcci_mnt/shihao/project/verl/psy_r1/SMHC_data_v4\"\n",
    "        },\n",
    "        # 可以添加更多数据集\n",
    "        # {\n",
    "        #     \"name\": \"additional_dataset\",\n",
    "        #     \"train_files\": [\"/path/to/additional_train.json\"],\n",
    "        #     \"val_files\": [\"/path/to/additional_val.json\"],\n",
    "        #     \"output_dir\": \"/path/to/additional_output\"\n",
    "        # }\n",
    "    ]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n处理数据集: {dataset['name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            train_df, val_df = convert_sharegpt_to_parquet(\n",
    "                train_files=dataset['train_files'],\n",
    "                val_files=dataset['val_files'], \n",
    "                output_dir=dataset['output_dir'],\n",
    "                remove_duplicates=True\n",
    "            )\n",
    "            print(f\"✓ {dataset['name']} 处理完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {dataset['name']} 处理失败: {e}\")\n",
    "\n",
    "# 数据质量检查函数\n",
    "def check_data_quality(files: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    检查数据质量\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        \"total_files\": len(files),\n",
    "        \"valid_files\": 0,\n",
    "        \"total_records\": 0,\n",
    "        \"patient_ids\": set(),\n",
    "        \"file_details\": []\n",
    "    }\n",
    "    \n",
    "    for file_path in files:\n",
    "        file_info = {\"path\": file_path, \"exists\": False, \"records\": 0, \"error\": None}\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                file_info[\"exists\"] = True\n",
    "                quality_report[\"valid_files\"] += 1\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    file_info[\"records\"] = len(data)\n",
    "                    quality_report[\"total_records\"] += len(data)\n",
    "                    \n",
    "                    # 收集patient_id\n",
    "                    for item in data:\n",
    "                        if 'patient_id' in item:\n",
    "                            quality_report[\"patient_ids\"].add(item['patient_id'])\n",
    "            else:\n",
    "                file_info[\"error\"] = \"文件不存在\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            file_info[\"error\"] = str(e)\n",
    "            \n",
    "        quality_report[\"file_details\"].append(file_info)\n",
    "    \n",
    "    quality_report[\"unique_patients\"] = len(quality_report[\"patient_ids\"])\n",
    "    return quality_report\n",
    "\n",
    "# 演示用法\n",
    "print(\"使用高级多文件处理:\")\n",
    "train_files, val_files = advanced_multi_file_processing()\n",
    "\n",
    "print(\"\\n数据质量检查:\")\n",
    "train_quality = check_data_quality(train_files)\n",
    "val_quality = check_data_quality(val_files)\n",
    "\n",
    "print(f\"训练数据质量报告:\")\n",
    "print(f\"  有效文件: {train_quality['valid_files']}/{train_quality['total_files']}\")\n",
    "print(f\"  总记录数: {train_quality['total_records']}\")\n",
    "print(f\"  唯一患者: {train_quality['unique_patients']}\")\n",
    "\n",
    "print(f\"验证数据质量报告:\")\n",
    "print(f\"  有效文件: {val_quality['valid_files']}/{val_quality['total_files']}\")\n",
    "print(f\"  总记录数: {val_quality['total_records']}\")\n",
    "print(f\"  唯一患者: {val_quality['unique_patients']}\")\n",
    "\n",
    "print(\"\\n要执行批量处理，取消注释下面的代码:\")\n",
    "print(\"# batch_process_multiple_datasets()\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
